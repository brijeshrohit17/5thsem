logical discussion
The logistic regression follows a s-shaped curve to the predict the feature of the a binary response variables. In the Logistic Regression model, model fitting is done using Chi-square instead of R2 as the statistic. Chi-square is a measure of the observed and the expected values. The objective is to find the minimum deviance between the observed and predicted values to obtain the best fitting line using calculus.

Decision tree is widely used in classification problems with a fixed target variable that can be applied to both categorical and continuous input and output variables. In this technique, the given data set is divided into two or more consistent sets based on most significant input variables which act as differentiator.

Random Forest constructs the number of decision trees on different sub-samples of the dataset and takes average to get the predictive accuracy and also controls overfitting of the model. For classification its output is based on mode of the classes and for regression it uses mean of the individual trees.

literature summary

A developer-centric bug prediction model by Dario Di Nucci, Fabio Palomba, Giuseppe De Rosa Gabriele Bavota, Rocco Oliveto and Andrea De Lucia

Cross-Project and Within-Project Semi-supervised Software Defect Prediction: A Unified approach, by F. Wu et al

Software metrics for fault prediction using machine learning approaches by Meiliana, S. Karim, H. L. H. S. Warnars, F. L. Gaol, E. Abdurachman and B. Soewito

The design of a software fault-prone application using evolutionary algorithm by M. M. Rosli, N. H. I. Teo, N. S. M. Yusop and N. S. Mohammad

An Approach for Software Defect Prediction by Combined Soft Computing by T P Pushphavathi

Various software metrics are used for predicting software faults and identifying the central aspect of the classification techniques. In this paper, machine learning algorithms like 1. Logistic Regression, 2. Decision Tree and Random Forest, and 3. Naive Bayes are used.

application

Application is to predict bugs not only in current source code but also that can arise in future based on historical data.

problem statement

In big software, bugs may occur, which are very hard to find and a big challenge for system efficiency, reliability, quality, maintenance cost and consistency. In this paper, the approach to predicting software bugs' occurrences using supervised machine learning algorithms is discussed.

data set

Data is collected from GitHub hosted projects. The data set created has the shape of 182962 rows and 113 columns. Continuous and categorical variables are defined. Followed by this Null value treatment, Outliers treatment, and Garbage value treatment have been done to create analytical dataset. The imbalanced data set is converted to balanced data set. Univariate and Bivariate data analysis have been done to find out what variables influence the training of the model.

tools

Python [rich data science package] to implement algorithms, SPSS by IBM as alternative of python, Java and python projects from github as bug datasets, Metric suite given by Chidamber and Kemerer to test quality of application.

Obje para

The performance of the model is validated by identify the accuracy score, confusion matrix, ROC-AUC curve designing, and Probability curve

Technical sil

 In Logistic Regression approach complex optimized equation is obtained by converting from the logistic equation to the OLS-type equation which is derived from probabilistic approach which is in the attached file. P is the probability for Y=1 and 1-P is the probability of getting Y=0. It is the fit of the observed values (Y) to the expected values (Y'). Objective is to find the minimum deviance between the observed and predicted values to obtain the best fitting line using calculus. With the help of machine learning algorithms, the computer derives the best fit by trying different iterations with different methodologies to find the smallest difference between the actual and predicted values.

In Decision Tree and Random Forest method we consider the classifier with N number of rows and M number of column or factors. The task is to split M input variables to find the node of the tree and M should be smaller than M. Than select the training dataset for each decision tree by selecting the n occurrences with replacement from all available training rows and use the other rows to test the algorithm accuracy. For each child of the tree, randomly choose m variables on which we can make the decision of the node. Use Information gain or Gini index prioritize the feature importance of the columns. The final classifier from multiple trees will be constructed using voting techniques. The highest of group of trees of a particular class will be prioritized.

In Naive Bayes method algorithm helps to understand Bayesian Belief Networks and the EM Algorithm. The fundamental assumptions of the naive Bayes theorem is that the classes are independent with each other and there will not be any correlation between the variables which is described in equation in attached file.

